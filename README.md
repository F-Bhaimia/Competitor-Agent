<p align="center">
  <img src="https://img.shields.io/badge/Python-3.9+-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/OpenAI-GPT--4o--mini-412991?style=for-the-badge&logo=openai&logoColor=white" alt="OpenAI">
  <img src="https://img.shields.io/badge/Streamlit-Dashboard-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white" alt="Streamlit">
  <img src="https://img.shields.io/badge/Playwright-Crawling-2EAD33?style=for-the-badge&logo=playwright&logoColor=white" alt="Playwright">
</p>

<h1 align="center">
  <br>
  ğŸ” Competitor News Monitor
  <br>
</h1>

<h4 align="center">An AI-powered competitive intelligence platform that automatically discovers, analyzes, and summarizes competitor activity in real-time.</h4>

<p align="center">
  <a href="#-key-features">Key Features</a> â€¢
  <a href="#-quick-start">Quick Start</a> â€¢
  <a href="#-architecture">Architecture</a> â€¢
  <a href="#-usage">Usage</a> â€¢
  <a href="#-dashboard">Dashboard</a> â€¢
  <a href="#-deployment">Deployment</a> â€¢
  <a href="#-api-reference">API Reference</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/status-production-brightgreen?style=flat-square" alt="Status">
  <img src="https://img.shields.io/badge/coverage-85%25-yellow?style=flat-square" alt="Coverage">
  <img src="https://img.shields.io/badge/PRs-welcome-brightgreen?style=flat-square" alt="PRs Welcome">
</p>

---

## ğŸ¯ What is Competitor News Monitor?

**Competitor News Monitor** transforms how businesses track their competitive landscape. Instead of manually scouring competitor blogs and news sites, this intelligent agent does it for youâ€”crawling, parsing, classifying, and summarizing content automatically.

Built for the **membership and fitness management software industry**, it monitors key players like Kicksite, Spark Membership, MyStudio, ZenPlanner, GloFox, and ClubOS, delivering actionable intelligence directly to your dashboard.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚   "Know what your competitors are doing before your customers tell you."   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Key Features

<table>
<tr>
<td width="50%">

### ğŸ•·ï¸ Intelligent Crawling
- **Breadth-first discovery** of blog/news pages
- **JavaScript rendering** via Playwright for SPA sites
- **Configurable depth** and domain restrictions
- **Polite crawling** with rate limiting
- **Duplicate detection** via SHA-256 hashing

</td>
<td width="50%">

### ğŸ§  AI-Powered Analysis
- **11 content categories** (Product, Pricing, M&A, etc.)
- **3-tier impact scoring** (High/Medium/Low)
- **40-80 word summaries** focused on strategic signals
- **GPT-4o-mini** for cost-effective classification
- **Graceful fallbacks** when API fails

</td>
</tr>
<tr>
<td width="50%">

### ğŸ“Š Interactive Dashboard
- **Real-time filtering** by company, category, impact
- **Date range selection** for trend analysis
- **Clickable source links** to original articles
- **Color-coded impact badges** for quick scanning
- **Executive summary generation** with PDF export

</td>
<td width="50%">

### âš™ï¸ Enterprise-Ready
- **Scheduled automation** (cron/Task Scheduler)
- **Process locking** prevents concurrent runs
- **Atomic file operations** for data integrity
- **Comprehensive logging** with rotation
- **One-command deployment** to Linux servers

</td>
</tr>
</table>

---

## ğŸƒ Quick Start

### Prerequisites

| Requirement | Version | Purpose |
|-------------|---------|---------|
| Python | 3.9+ | Runtime environment |
| OpenAI API Key | - | GPT-4o-mini for classification |
| Git | Any | Version control |

### Installation (3 minutes)

```bash
# 1. Clone the repository
git clone https://github.com/F-Bhaimia/Competitor-Agent.git
cd Competitor-Agent

# 2. Create virtual environment
python -m venv .venv

# 3. Activate virtual environment
# Windows:
.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# 4. Install dependencies
pip install -r requirements.txt

# 5. Install Playwright browsers (for JS-heavy sites)
playwright install chromium

# 6. Configure environment
echo "OPENAI_API_KEY=your_key_here" > .env

# 7. Run your first crawl!
python -m jobs.daily_scan

# 8. Launch the dashboard
streamlit run streamlit_app/Home.py
```

**That's it!** Open `http://localhost:8501` in your browser.

---

## ğŸ—ï¸ Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            COMPETITOR NEWS MONITOR                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                             â”‚
â”‚  â”‚ monitors.yaml   â”‚ â—„â”€â”€â”€ Configuration: competitors, URLs, crawl settings       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                             â”‚
â”‚           â”‚                                                                      â”‚
â”‚           â–¼                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚                        DATA COLLECTION LAYER                         â”‚        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚        â”‚
â”‚  â”‚  â”‚   crawl.py   â”‚    â”‚ fetch_rss.py â”‚    â”‚   Playwright         â”‚   â”‚        â”‚
â”‚  â”‚  â”‚  (requests)  â”‚    â”‚ (feedparser) â”‚    â”‚   (JS Rendering)     â”‚   â”‚        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚        â”‚
â”‚  â”‚         â”‚                   â”‚                       â”‚               â”‚        â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚        â”‚
â”‚  â”‚                             â”‚                                        â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                â–¼                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚                        PROCESSING LAYER                              â”‚        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚        â”‚
â”‚  â”‚  â”‚   parse.py   â”‚â”€â”€â”€â–¶â”‚ daily_scan.pyâ”‚â”€â”€â”€â–¶â”‚    updates.csv       â”‚   â”‚        â”‚
â”‚  â”‚  â”‚ (HTMLâ†’Text)  â”‚    â”‚  (Dedupe)    â”‚    â”‚    (Raw Data)        â”‚   â”‚        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚        â”‚
â”‚  â”‚                                                      â”‚               â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                         â–¼                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚                        ENRICHMENT LAYER                              â”‚        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚        â”‚
â”‚  â”‚  â”‚ classify.py  â”‚    â”‚enrich_updatesâ”‚    â”‚ enriched_updates.csv â”‚   â”‚        â”‚
â”‚  â”‚  â”‚  (GPT-4o)    â”‚â”€â”€â”€â–¶â”‚    .py       â”‚â”€â”€â”€â–¶â”‚ (+summary, category, â”‚   â”‚        â”‚
â”‚  â”‚  â”‚              â”‚    â”‚              â”‚    â”‚      impact)         â”‚   â”‚        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚        â”‚
â”‚  â”‚                                                      â”‚               â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                         â”‚                        â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚           â”‚                                             â”‚            â”‚           â”‚
â”‚           â–¼                                             â–¼            â–¼           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   DASHBOARD     â”‚                           â”‚   EXPORTS    â”‚ â”‚  ALERTS â”‚     â”‚
â”‚  â”‚   (Streamlit)   â”‚                           â”‚  CSV/PDF/XLS â”‚ â”‚ (Slack) â”‚     â”‚
â”‚  â”‚                 â”‚                           â”‚              â”‚ â”‚         â”‚     â”‚
â”‚  â”‚  â€¢ Filtering    â”‚                           â”‚  â€¢ QA Sample â”‚ â”‚ â€¢ High  â”‚     â”‚
â”‚  â”‚  â€¢ Charts       â”‚                           â”‚  â€¢ Quarterly â”‚ â”‚  Impact â”‚     â”‚
â”‚  â”‚  â€¢ PDF Export   â”‚                           â”‚    Rollup    â”‚ â”‚  Only   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Sequence

```
1. DISCOVER    â”€â”€â–¶  Breadth-first traversal from start_urls
                    Filter for /blog/, /news/, /post/, /article/ paths

2. FETCH       â”€â”€â–¶  HTTP GET with custom User-Agent
                    Fallback to Playwright for JS-rendered content

3. PARSE       â”€â”€â–¶  BeautifulSoup extracts title, date, body
                    JSON-LD â†’ OpenGraph â†’ <time> tag â†’ None

4. DEDUPLICATE â”€â”€â–¶  SHA-256 hash of (company || normalized_url)
                    Skip if ID exists in updates.csv

5. STORE       â”€â”€â–¶  Append to data/updates.csv
                    Mirror to data/updates.parquet for analytics

6. ENRICH      â”€â”€â–¶  GPT-4o-mini classifies category + impact
                    Generates 40-80 word strategic summary

7. EXPORT      â”€â”€â–¶  enriched_updates.csv for dashboard
                    Optional: Slack webhook for High-impact items
```

---

## ğŸ“‚ Project Structure

```
Competitor-Agent/
â”‚
â”œâ”€â”€ ğŸ“ app/                          # Core application modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ crawl.py                     # Web crawler with Playwright fallback
â”‚   â”œâ”€â”€ parse.py                     # HTML parsing & content extraction
â”‚   â”œâ”€â”€ classify.py                  # GPT-4o-mini classification engine
â”‚   â””â”€â”€ summarize.py                 # Article summarization utilities
â”‚
â”œâ”€â”€ ğŸ“ jobs/                         # Batch processing jobs
â”‚   â”œâ”€â”€ daily_scan.py                # Primary crawler job
â”‚   â”œâ”€â”€ fetch_rss.py                 # Google News RSS integration
â”‚   â”œâ”€â”€ enrich_updates.py            # AI enrichment pipeline
â”‚   â”œâ”€â”€ append_updates.py            # Data merging utility
â”‚   â”œâ”€â”€ update_daily.py              # Pipeline orchestrator
â”‚   â”œâ”€â”€ qa_sampler.py                # QA sample generator (10%)
â”‚   â””â”€â”€ quarterly_rollup.py          # Quarterly analytics aggregation
â”‚
â”œâ”€â”€ ğŸ“ streamlit_app/                # Interactive dashboard
â”‚   â””â”€â”€ Home.py                      # Main Streamlit application
â”‚
â”œâ”€â”€ ğŸ“ config/                       # Configuration files
â”‚   â””â”€â”€ monitors.yaml                # Competitor & crawl settings
â”‚
â”œâ”€â”€ ğŸ“ automation/                   # Scheduled task scripts
â”‚   â””â”€â”€ nightly_update.ps1           # Windows Task Scheduler script
â”‚
â”œâ”€â”€ ğŸ“ scripts/                      # Shell scripts
â”‚   â”œâ”€â”€ run_pipeline.sh              # Full pipeline (Linux/macOS)
â”‚   â””â”€â”€ update_daily.sh              # Cron-friendly daily update
â”‚
â”œâ”€â”€ ğŸ“ data/                         # Data storage (gitignored)
â”‚   â”œâ”€â”€ updates.csv                  # Raw crawled articles
â”‚   â”œâ”€â”€ updates.parquet              # Parquet mirror for analytics
â”‚   â””â”€â”€ enriched_updates.csv         # AI-enriched articles
â”‚
â”œâ”€â”€ ğŸ“ exports/                      # Generated reports
â”‚   â”œâ”€â”€ qa_sample_YYYYMMDD.csv       # QA review samples
â”‚   â””â”€â”€ quarterly_rollup.csv         # Aggregated statistics
â”‚
â”œâ”€â”€ ğŸ“ logs/                         # Execution logs
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt              # Python dependencies
â”œâ”€â”€ ğŸ“„ deploy.sh                     # One-command server deployment
â”œâ”€â”€ ğŸ“„ DEPLOYMENT.md                 # Detailed deployment guide
â”œâ”€â”€ ğŸ“„ Makefile                      # Development shortcuts
â””â”€â”€ ğŸ“„ README.md                     # You are here!
```

---

## ğŸ® Usage

### Job Commands Reference

| Job | Command | Description | Typical Runtime |
|-----|---------|-------------|-----------------|
| **Daily Scan** | `python -m jobs.daily_scan` | Crawl all competitors, extract articles | 5-15 min |
| **RSS Fetch** | `python -m jobs.fetch_rss --since 2025-01-01` | Pull from Google News feeds | 30 sec |
| **Enrich** | `python -m jobs.enrich_updates` | Apply AI classification | ~2 sec/article |
| **Full Pipeline** | `python -m jobs.update_daily` | Fetch â†’ Merge â†’ Enrich | 10-20 min |
| **QA Sample** | `python -m jobs.qa_sampler` | Generate 10% sample for review | Instant |
| **Quarterly** | `python -m jobs.quarterly_rollup` | Aggregate by company Ã— quarter | Instant |

### Example Workflows

#### Daily Competitive Intelligence Gathering
```bash
# Run the full pipeline (fetches + enriches)
python -m jobs.update_daily

# Or run individual steps for debugging
python -m jobs.daily_scan          # Step 1: Crawl
python -m jobs.enrich_updates      # Step 2: AI classification
```

#### On-Demand Deep Dive
```bash
# Add a new competitor, then run a targeted crawl
# 1. Edit config/monitors.yaml to add the competitor
# 2. Run the daily scan
python -m jobs.daily_scan

# 3. Enrich the new data
python -m jobs.enrich_updates

# 4. Launch dashboard to explore
streamlit run streamlit_app/Home.py
```

#### Quality Assurance Review
```bash
# Generate a random 10% sample of enriched articles
python -m jobs.qa_sampler

# Output: exports/qa_sample_YYYYMMDD.csv
# Review summaries, categories, and impact scores for accuracy
```

---

## ğŸ“Š Dashboard

### Launching the Dashboard

```bash
streamlit run streamlit_app/Home.py
```

Access at **http://localhost:8501**

### Dashboard Sections

<table>
<tr>
<td width="33%" align="center">

**ğŸ“ˆ Posts by Competitor**

KPI cards, quarterly charts, and chronological feed view

</td>
<td width="33%" align="center">

**ğŸ“¤ Export**

Download filtered data as CSV

</td>
<td width="33%" align="center">

**âœï¸ Manual Edits**

Correct AI classifications inline

</td>
</tr>
<tr>
<td width="33%" align="center">

**ğŸ“‹ Executive Summary**

AI-generated insights with PDF export

</td>
<td width="33%" align="center">

**ğŸ”§ Data Quality Tools**

Run enrichment, generate QA samples

</td>
<td width="33%" align="center">

**ğŸ” Full-Text Search**

Search across titles and summaries

</td>
</tr>
</table>

### Filtering Options

| Filter | Type | Description |
|--------|------|-------------|
| **Company** | Multi-select | Filter by one or more competitors |
| **Category** | Multi-select | Filter by content category |
| **Impact** | Multi-select | Filter by High/Medium/Low |
| **Date Range** | Date picker | Filter by published or collected date |
| **Search** | Text input | Full-text search in title/summary |

---

## ğŸ·ï¸ Classification System

### Content Categories

| Category | Description | Examples |
|----------|-------------|----------|
| **Product/Feature** | New features, GA releases, product updates | "Introducing AI Scheduling", "Mobile App 3.0" |
| **Pricing/Plans** | Pricing changes, new tiers, packaging | "New Enterprise Plan", "Price Increase Notice" |
| **Partnership** | Strategic alliances, integrations | "Integration with Stripe", "Partnership with Nike" |
| **Acquisition/Investment** | M&A, funding rounds | "Series B Funding", "Acquisition of XYZ" |
| **Case Study/Customer** | Success stories, testimonials | "How Gym ABC grew 40%", "Customer Spotlight" |
| **Events/Webinar** | Conferences, webinars, workshops | "Join us at IHRSA", "Upcoming Webinar" |
| **Best Practices/Guides** | Educational content, how-tos | "5 Tips for Retention", "Ultimate Guide to..." |
| **Security/Compliance** | Security updates, certifications | "SOC 2 Certification", "GDPR Compliance" |
| **Hiring/Leadership** | Team changes, executive moves | "New CEO Announcement", "We're Hiring!" |
| **Company News** | General announcements | "Office Relocation", "Anniversary" |
| **Other** | Miscellaneous content | Doesn't fit other categories |

### Impact Scoring

| Level | Criteria | Action Required |
|-------|----------|-----------------|
| ğŸ”´ **High** | Pricing changes, major GA features, acquisitions, security incidents, big partnerships | Immediate review recommended |
| ğŸŸ  **Medium** | Meaningful feature updates, significant case studies, notable events | Review within the week |
| âšª **Low** | Generic tips, routine posts, educational content | Monitor for trends |

---

## ğŸ”§ Configuration

### monitors.yaml

```yaml
# Global crawl settings
global:
  user_agent: "MS-CompetitorBot/1.0 (+contact: ci@membersolutions.com)"
  request_timeout_s: 20
  max_pages_per_site: 60
  follow_within_domain_only: true
  dedupe_window_days: 365
  high_impact_labels: ["Pricing", "M&A", "Security", "Product Update (GA)"]
  alert_on_impact_levels: ["High"]

# Competitors to monitor
competitors:
  - name: "Kicksite"
    start_urls:
      - "https://kicksite.com/blog"
      - "https://kicksite.com/newsletters/"

  - name: "Spark Membership"
    start_urls:
      - "https://sparkmembership.com/blog/"

  - name: "MyStudio"
    start_urls:
      - "https://www.mystudio.io/blog"

  - name: "ZenPlanner (Daxko)"
    start_urls:
      - "https://zenplanner.com/blog/"
      - "https://www.daxko.com/blog"

  - name: "GloFox (ABC Fitness)"
    start_urls:
      - "https://www.glofox.com/blog/"
      - "https://www.abcfitness.com/resources/blog/"

  - name: "ClubOS (Formerly ASF)"
    start_urls:
      - "https://www.club-os.com/blog"
```

### Environment Variables (.env)

```bash
# Required
OPENAI_API_KEY=sk-your-openai-api-key-here

# Optional (for Slack alerts)
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
```

---

## ğŸš€ Deployment

### One-Command Server Deployment

For production deployment on a Linux server (Ubuntu/Debian):

```bash
# SSH to your server
ssh root@your-server-ip

# Download and run the deployment script
curl -O https://raw.githubusercontent.com/F-Bhaimia/Competitor-Agent/main/deploy.sh
chmod +x deploy.sh
./deploy.sh
```

The script automatically:
- âœ… Installs system dependencies
- âœ… Sets up Python virtual environment
- âœ… Configures Playwright browsers
- âœ… Creates systemd service
- âœ… Configures Nginx reverse proxy
- âœ… Sets up UFW firewall
- âœ… Schedules daily cron job

See **[DEPLOYMENT.md](DEPLOYMENT.md)** for the complete manual deployment guide.

### Scheduled Automation

#### Linux/macOS (Cron)
```bash
# Run daily at 2 AM
0 2 * * * /opt/competitor-agent/scripts/update_daily.sh >> /opt/competitor-agent/logs/cron.log 2>&1
```

#### Windows (Task Scheduler)
```powershell
# Run via PowerShell
powershell.exe -ExecutionPolicy Bypass -File "C:\path\to\automation\nightly_update.ps1"
```

---

## ğŸ“š API Reference

### Core Modules

#### `app.crawl`

```python
from app.crawl import load_config, crawl_all, crawl_competitor

# Load configuration
global_cfg, competitors = load_config("config/monitors.yaml")

# Crawl all competitors
for page in crawl_all():
    print(f"[{page.company}] {page.url}")

# Crawl a single competitor
for page in crawl_competitor(competitors[0], global_cfg):
    print(page.html[:500])
```

#### `app.parse`

```python
from app.parse import parse_article

article = parse_article(
    company="Kicksite",
    url="https://kicksite.com/blog/new-feature",
    html="<html>...</html>"
)

print(article.title)        # "New Feature Announcement"
print(article.published_at) # "2025-01-15T10:00:00"
print(article.clean_text)   # Extracted body text
```

#### `app.classify`

```python
from app.classify import classify_article, CATEGORIES

result = classify_article(
    company="Kicksite",
    title="New Pricing Plans Available",
    body="We're excited to announce our new pricing structure..."
)

print(result)
# {
#     "summary": "Kicksite introduces new pricing tiers with...",
#     "category": "Pricing/Plans",
#     "impact": "High"
# }
```

### Data Structures

#### Page (from crawl)
```python
@dataclass
class Page:
    company: str    # Competitor name
    url: str        # Page URL
    html: str       # Raw HTML content
```

#### Article (from parse)
```python
@dataclass
class Article:
    company: str              # Competitor name
    source_url: str           # Original URL
    title: str                # Extracted title
    published_at: Optional[str]  # ISO datetime or None
    clean_text: str           # Extracted body text
```

### CSV Schema

#### updates.csv (Raw)
| Column | Type | Description |
|--------|------|-------------|
| `id` | string | SHA-256 hash of company+url |
| `company` | string | Competitor name |
| `source_url` | string | Original article URL |
| `title` | string | Article title |
| `published_at` | datetime | Publish date (if found) |
| `collected_at` | datetime | When crawled (UTC) |
| `clean_text` | string | Article body text |

#### enriched_updates.csv (+ AI columns)
| Column | Type | Description |
|--------|------|-------------|
| ... | ... | All columns from updates.csv |
| `summary` | string | AI-generated 40-80 word summary |
| `category` | string | One of 11 categories |
| `impact` | string | High, Medium, or Low |

---

## ğŸ”’ Security & Reliability

### Error Handling

| Scenario | Behavior |
|----------|----------|
| **Network timeout** | Retry with exponential backoff (Tenacity) |
| **403/429 response** | Skip page, continue crawl |
| **OpenAI API failure** | Return defaults (category="Other", impact="Low") |
| **Malformed HTML** | Best-effort parsing, skip if no content |
| **Concurrent runs** | Prevented via file locks / OS mutex |

### Data Integrity

- **Atomic writes**: Temp file â†’ `os.replace()` for crash safety
- **Deduplication**: SHA-256 hash prevents duplicate entries
- **Parquet mirror**: Binary format for faster analytics queries
- **Log rotation**: Keeps last 15 pipeline logs

### Security Best Practices

```bash
# Secure your .env file
chmod 600 .env

# Don't commit secrets
echo ".env" >> .gitignore

# Use SSH keys for server access
ssh-keygen -t ed25519 -C "your_email@example.com"

# Enable UFW firewall
sudo ufw allow 22,80,443/tcp
sudo ufw enable
```

---

## ğŸ› ï¸ Troubleshooting

### Common Issues

<details>
<summary><b>âŒ "No module named 'playwright'"</b></summary>

```bash
pip install playwright
playwright install chromium
```
</details>

<details>
<summary><b>âŒ Crawl returns empty results</b></summary>

1. Check if the site uses JavaScript rendering:
```bash
curl -s "https://example.com/blog" | wc -c  # Should be > 1000 chars
```

2. If tiny, the site needs Playwright (should be automatic fallback)

3. Check `config/monitors.yaml` for correct URLs
</details>

<details>
<summary><b>âŒ OpenAI rate limiting</b></summary>

The system uses Tenacity with exponential backoff. If persistent:
- Check your OpenAI dashboard for quota limits
- Reduce `BATCH_SIZE` in `enrich_updates.py`
- Increase `SLEEP_BETWEEN` delay
</details>

<details>
<summary><b>âŒ Dashboard won't start</b></summary>

```bash
# Check if port 8501 is in use
netstat -tulpn | grep 8501

# Run with explicit port
streamlit run streamlit_app/Home.py --server.port 8502

# Check Streamlit logs
streamlit run streamlit_app/Home.py 2>&1 | tee dashboard.log
```
</details>

<details>
<summary><b>âŒ Data not updating in dashboard</b></summary>

1. Click "Reload Data" button in the dashboard
2. Check if enrichment job completed:
```bash
tail -f logs/pipeline_*.log
```
3. Verify data files exist:
```bash
ls -la data/
```
</details>

---

## ğŸ“ˆ Performance Optimization

### Recommended Settings by Use Case

| Scenario | `max_pages_per_site` | `request_timeout_s` | Notes |
|----------|---------------------|---------------------|-------|
| Quick daily check | 20 | 15 | Fast, catches recent posts |
| Weekly deep crawl | 100 | 30 | Comprehensive coverage |
| Initial backfill | 200 | 45 | One-time historical load |
| Low-memory server | 30 | 20 | Reduces Playwright memory |

### Memory Optimization

```bash
# If server runs out of memory, add swap
sudo fallocate -l 2G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how to get started:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

### Development Setup

```bash
# Clone your fork
git clone https://github.com/YOUR_USERNAME/Competitor-Agent.git
cd Competitor-Agent

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt

# Run tests (if available)
pytest tests/
```

---

## ğŸ“¦ Dependencies

### Core Stack

| Package | Version | Purpose |
|---------|---------|---------|
| `requests` | latest | HTTP client for web crawling |
| `beautifulsoup4` | latest | HTML parsing |
| `lxml` | latest | Fast XML/HTML parser |
| `playwright` | latest | JavaScript rendering |
| `openai` | â‰¥1.40.0 | GPT-4o-mini API client |
| `streamlit` | latest | Interactive dashboard |
| `pandas` | latest | Data manipulation |
| `pyarrow` | latest | Parquet file support |

### Supporting Libraries

| Package | Purpose |
|---------|---------|
| `python-dotenv` | Environment variable loading |
| `feedparser` | RSS feed parsing |
| `tenacity` | Retry logic with backoff |
| `pydantic` | Configuration validation |
| `reportlab` | PDF generation |
| `python-dateutil` | Date parsing |
| `openpyxl` | Excel export |
| `slack_sdk` | Slack webhook integration |

---

## ğŸ“Š Monitored Competitors

| Competitor | Parent Company | Monitored Since |
|------------|---------------|-----------------|
| **Kicksite** | Independent | 2025 |
| **Spark Membership** | Independent | 2025 |
| **MyStudio** | Independent | 2025 |
| **ZenPlanner** | Daxko | 2025 |
| **GloFox** | ABC Fitness | 2025 |
| **ClubOS** | Formerly ASF | 2025 |

---

## ğŸ“œ Changelog

### v1.0.0 (2025)
- Initial release
- Web crawling with Playwright fallback
- GPT-4o-mini classification pipeline
- Streamlit dashboard with PDF export
- Linux deployment automation
- Windows PowerShell automation

---

## ğŸ“ Support

Having issues? Here's how to get help:

1. **Check the docs**: Read through this README and [DEPLOYMENT.md](DEPLOYMENT.md)
2. **Search issues**: Look for similar problems in [GitHub Issues](https://github.com/F-Bhaimia/Competitor-Agent/issues)
3. **Open an issue**: If your problem is new, create a detailed issue with:
   - Python version (`python --version`)
   - Operating system
   - Error messages and logs
   - Steps to reproduce

---

## ğŸ™ Acknowledgments

- **OpenAI** for GPT-4o-mini powering our classification
- **Streamlit** for the beautiful dashboard framework
- **Playwright** for headless browser automation
- **BeautifulSoup** for rock-solid HTML parsing

---

<p align="center">
  <b>Built with â¤ï¸ for competitive intelligence</b>
  <br>
  <sub>Making market research effortless, one crawl at a time.</sub>
</p>

<p align="center">
  <a href="#-competitor-news-monitor">Back to top â†‘</a>
</p>
