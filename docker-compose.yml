# Competitor News Monitor - Docker Compose
# Usage:
#   docker compose up -d          # Start all services
#   docker compose logs -f        # View logs
#   docker compose down           # Stop all services
#   docker compose pull && docker compose up -d --build  # Update

services:
  dashboard:
    build: .
    container_name: competitor-dashboard
    restart: unless-stopped
    ports:
      - "127.0.0.1:8501:8501"  # Only expose to localhost (nginx proxies to 80/443)
    volumes:
      - ./data:/opt/competitor-agent/data
      - ./logs:/opt/competitor-agent/logs
      - ./exports:/opt/competitor-agent/exports
      - ./config:/opt/competitor-agent/config
      - ./.env:/opt/competitor-agent/.env:ro
    environment:
      - PYTHONUNBUFFERED=1
    command: streamlit run streamlit_app/Home.py --server.port=8501 --server.address=0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  webhook:
    build: .
    container_name: competitor-webhook
    restart: unless-stopped
    ports:
      - "127.0.0.1:8001:8001"  # Only expose to localhost (nginx proxies /email)
    volumes:
      - ./data:/opt/competitor-agent/data
      - ./logs:/opt/competitor-agent/logs
      - ./config:/opt/competitor-agent/config
      - ./.env:/opt/competitor-agent/.env:ro
    environment:
      - PYTHONUNBUFFERED=1
    command: python -m app.webhook_server
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Daily pipeline runner (RSS fetch + enrichment)
  # Runs once then exits - use cron to schedule
  pipeline:
    build: .
    container_name: competitor-pipeline
    profiles: ["jobs"]  # Only runs when explicitly called
    volumes:
      - ./data:/opt/competitor-agent/data
      - ./logs:/opt/competitor-agent/logs
      - ./config:/opt/competitor-agent/config
      - ./.env:/opt/competitor-agent/.env:ro
    environment:
      - PYTHONUNBUFFERED=1
    command: >
      sh -c "python -m jobs.fetch_rss && python -m jobs.enrich_updates"

  # Blog crawler (Playwright-based)
  # Runs once then exits - use cron to schedule
  crawler:
    build: .
    container_name: competitor-crawler
    profiles: ["jobs"]  # Only runs when explicitly called
    volumes:
      - ./data:/opt/competitor-agent/data
      - ./logs:/opt/competitor-agent/logs
      - ./config:/opt/competitor-agent/config
      - ./.env:/opt/competitor-agent/.env:ro
    environment:
      - PYTHONUNBUFFERED=1
    command: python -m jobs.daily_scan
